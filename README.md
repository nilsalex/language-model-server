# language-model-server

A small server for talking to the [google FLAN-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5) models.

## Usage

1) Create a [venv](https://docs.python.org/3/library/venv.html)
   ```
   $ python3 -m venv ./.venv
   $ source ./.venv/bin/activate
   ```
2) Install the dependencies
   ```
   $ pip install --upgrade pip
   $ pip install -r requirements.txt
   ```
3) Run the server, for example for the `small` model:
   ```
   $ python3 ./flan_t5_server.py google/flan-t5-small run
   ```
4) The server now listens at port `5000`. Generate predictions by sending an HTTP request
   with form data (`application/x-www-form-urlencoded` or `multipart/form-data`) to the endpoint `/`.

   The only required field is `text` for the prompt.

   Optionally you can pass `max_tokens` for the maximum
   number of tokens generated by the model. The response is plain text (`text/plain`).
   ```
   $ curl -w "\n" -XPOST http://localhost:5000/ -d max_tokens=100 -d text="Can the baby speak?"
   no
   ```
5) If you only want to download the models, you can instead run
   ```
   $ python3 ./flan_t5_server.py google/flan-t5-small init
   ```

## Docker build

The [Dockerfile](./Dockerfile) can be used to build a GPU enabled image
for different model sizes:

```
$ docker build \
    --build-arg MODEL_NAME=google/flan-t5-small \
    -t language-model-server:small \
    .
$ docker run -p 5000:5000 language-model-server:small
```

Valid values for `MODEL_NAME` are

- `google/flan-t5-small`
- `google/flan-t5-base`
- `google/flan-t5-large`
- `google/flan-t5-xl`
- `google/flan-t5-xxl`

Note that the `small` model requires about 16 Gi RAM, the `xxl` model up to 64 Gi.